{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Model Results\n",
    "\n",
    "This notebook analyzes the output of the GMRF model, loading posterior samples to visualize probabilities and model parameters.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import functools\n",
    "from dataloader import load_data\n",
    "from knn_utils import load_condition_knn\n",
    "\n",
    "# --- Global Context and Caching ---\n",
    "\n",
    "@functools.lru_cache(maxsize=None)\n",
    "def get_global_condition_info():\n",
    "    \"\"\"Loads the full list of conditions and their KNN graph.\"\"\"\n",
    "    print(\"Loading global condition info (condition list and KNN)...\")\n",
    "    _, _, full_condition_list = load_data() # Load all data to get the full list\n",
    "    condition_knn = load_condition_knn()\n",
    "    \n",
    "    # Create a mapping from condition name to its global index\n",
    "    condition_to_idx = {name: i for i, name in enumerate(full_condition_list)}\n",
    "    \n",
    "    print(\"Global condition info loaded.\")\n",
    "    return full_condition_list, condition_knn['indices'], condition_to_idx\n",
    "\n",
    "_reconstituted_draws_cache = {}\n",
    "\n",
    "# --- Main Loading and Interpolation Logic ---\n",
    "\n",
    "def _get_shard_specific_conditions(shard_dir):\n",
    "    \"\"\"\n",
    "    Placeholder: This function would ideally determine the specific conditions\n",
    "    present in a given shard. For now, we assume the number of columns in Lambda\n",
    "    corresponds to a slice of the global condition list.\n",
    "    \"\"\"\n",
    "    # This is a simplification. A more robust solution would save condition metadata in each shard.\n",
    "    samples = np.load(os.path.join(shard_dir, 'mcmc_samples.npy'), allow_pickle=True).item()\n",
    "    num_conditions_in_shard = samples['Lambda'].shape[2]\n",
    "    full_list, _, _ = get_global_condition_info()\n",
    "    return full_list[:num_conditions_in_shard]\n",
    "\n",
    "def _interpolate_missing_lambdas(lambda_draws, knn_indices, condition_to_idx):\n",
    "    \"\"\"\n",
    "    Fills NaN values in the Lambda draws using nearest-neighbor interpolation.\n",
    "    \"\"\"\n",
    "    print(\"Performing nearest-neighbor interpolation for missing Lambda values...\")\n",
    "    num_draws, num_patients, num_conditions = lambda_draws.shape\n",
    "    \n",
    "    # Convert to numpy for nan-aware indexing and assignment\n",
    "    lambda_draws_np = np.asarray(lambda_draws)\n",
    "    \n",
    "    # Find all patient/condition pairs that are missing\n",
    "    missing_indices = np.argwhere(np.isnan(lambda_draws_np[0, :, :]))\n",
    "\n",
    "    for pat_idx, cond_idx in tqdm(missing_indices, desc=\"Interpolating\"):\n",
    "        # Find nearest neighbors for this condition\n",
    "        neighbor_g_indices = knn_indices[cond_idx, :]\n",
    "        \n",
    "        # Find the first neighbor that has a valid value for this patient\n",
    "        for neighbor_idx in neighbor_g_indices:\n",
    "            if not np.isnan(lambda_draws_np[0, pat_idx, neighbor_idx]):\n",
    "                # Found a valid neighbor, use its value for all draws for this patient\n",
    "                lambda_draws_np[:, pat_idx, cond_idx] = lambda_draws_np[:, pat_idx, neighbor_idx]\n",
    "                break\n",
    "    \n",
    "    print(\"Interpolation complete.\")\n",
    "    return jnp.asarray(lambda_draws_np)\n",
    "\n",
    "def load_reconstituted_lambda_draws(base_dir: str = 'Res/'):\n",
    "    \"\"\"\n",
    "    Loads Lambda draws from shards, pads them to a common shape, concatenates,\n",
    "    and then uses nearest-neighbor interpolation to fill missing values.\n",
    "    \"\"\"\n",
    "    cache_key = 'lambda_draws_interpolated'\n",
    "    if cache_key in _reconstituted_draws_cache:\n",
    "        print(\"Returning cached reconstituted Lambda draws.\")\n",
    "        return _reconstituted_draws_cache[cache_key]\n",
    "\n",
    "    print(\"Loading, padding, and interpolating Lambda draws from shards...\")\n",
    "    full_condition_list, knn_indices, condition_to_idx = get_global_condition_info()\n",
    "    C_full = len(full_condition_list)\n",
    "    \n",
    "    shard_dirs = sorted(glob.glob(os.path.join(base_dir, 'gmrf_*_shard_*')))\n",
    "    if not shard_dirs:\n",
    "        raise FileNotFoundError(f\"No 'gmrf_*' shard directories found in '{base_dir}'.\")\n",
    "\n",
    "    # Determine minimum number of draws across all shards first\n",
    "    shard_samples_info = [np.load(os.path.join(d, 'mcmc_samples.npy'), allow_pickle=True).item() for d in shard_dirs]\n",
    "    min_draws = min(s['Lambda'].shape[0] for s in shard_samples_info)\n",
    "    print(f\"Found {len(shard_dirs)} shards. Using {min_draws} draws from each.\")\n",
    "\n",
    "    padded_shards = []\n",
    "    for shard_dir, shard_samples in zip(shard_dirs, shard_samples_info):\n",
    "        lambda_shard = shard_samples['Lambda'][:min_draws, :, :]\n",
    "        S, I_shard, C_shard = lambda_shard.shape\n",
    "        \n",
    "        # This is the crucial simplification: we assume the shard's conditions\n",
    "        # are the first C_shard conditions of the global list.\n",
    "        shard_conditions = full_condition_list[:C_shard]\n",
    "        \n",
    "        # Create a full-size NaN array and fill it\n",
    "        padded_lambda = np.full((S, I_shard, C_full), np.nan)\n",
    "        for c_idx_shard, cond_name in enumerate(shard_conditions):\n",
    "            c_idx_global = condition_to_idx[cond_name]\n",
    "            padded_lambda[:, :, c_idx_global] = lambda_shard[:, :, c_idx_shard]\n",
    "        \n",
    "        padded_shards.append(padded_lambda)\n",
    "\n",
    "    # Concatenate along the patient axis\n",
    "    reconstituted_padded = np.concatenate(padded_shards, axis=1)\n",
    "    \n",
    "    # Interpolate NaN values\n",
    "    reconstituted_final = _interpolate_missing_lambdas(reconstituted_padded, knn_indices, condition_to_idx)\n",
    "    \n",
    "    _reconstituted_draws_cache[cache_key] = reconstituted_final\n",
    "    print(\"Reconstitution and interpolation complete.\")\n",
    "    \n",
    "    return reconstituted_final\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading, padding, and interpolating Lambda draws from shards...\n",
      "Loading global condition info (condition list and KNN)...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "load_data() got an unexpected keyword argument 'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[32m1.0\u001b[39m / (\u001b[32m1.0\u001b[39m + jnp.exp(-x))\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Load the reconstituted Lambda draws\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# This function handles the logic of finding shards and concatenating Lambdas\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m lambda_draws = \u001b[43mload_reconstituted_lambda_draws\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Compute posterior mean probabilities\u001b[39;00m\n\u001b[32m     14\u001b[39m P_mean = _sigmoid(jnp.mean(lambda_draws, axis=\u001b[32m0\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 79\u001b[39m, in \u001b[36mload_reconstituted_lambda_draws\u001b[39m\u001b[34m(base_dir)\u001b[39m\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _reconstituted_draws_cache[cache_key]\n\u001b[32m     78\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading, padding, and interpolating Lambda draws from shards...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m full_condition_list, knn_indices, condition_to_idx = \u001b[43mget_global_condition_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     80\u001b[39m C_full = \u001b[38;5;28mlen\u001b[39m(full_condition_list)\n\u001b[32m     82\u001b[39m shard_dirs = \u001b[38;5;28msorted\u001b[39m(glob.glob(os.path.join(base_dir, \u001b[33m'\u001b[39m\u001b[33mgmrf_*_shard_*\u001b[39m\u001b[33m'\u001b[39m)))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 16\u001b[39m, in \u001b[36mget_global_condition_info\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Loads the full list of conditions and their KNN graph.\"\"\"\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mLoading global condition info (condition list and KNN)...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m _, _, full_condition_list = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Load all data to get the full list\u001b[39;00m\n\u001b[32m     17\u001b[39m condition_knn = load_condition_knn()\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Create a mapping from condition name to its global index\u001b[39;00m\n",
      "\u001b[31mTypeError\u001b[39m: load_data() got an unexpected keyword argument 'batch_size'"
     ]
    }
   ],
   "source": [
    "# Heatmap of posterior mean probabilities\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "def _sigmoid(x: jnp.ndarray) -> jnp.ndarray:\n",
    "    return 1.0 / (1.0 + jnp.exp(-x))\n",
    "\n",
    "# Load the reconstituted Lambda draws\n",
    "# This function handles the logic of finding shards and concatenating Lambdas\n",
    "lambda_draws = load_reconstituted_lambda_draws()\n",
    "\n",
    "# Compute posterior mean probabilities\n",
    "P_mean = _sigmoid(jnp.mean(lambda_draws, axis=0))\n",
    "print(f\"Computed posterior mean probability matrix with shape: {P_mean.shape}\")\n",
    "\n",
    "# Plot heatmap in original order\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(np.asarray(P_mean), cmap='viridis', vmin=0.0, vmax=1.0, cbar=True,\n",
    "            xticklabels=False, yticklabels=False)\n",
    "plt.title('Posterior Mean Probability (Reconstituted from Shards)')\n",
    "plt.xlabel('Diseases')\n",
    "plt.ylabel('Patients')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of posterior standard deviation of probabilities\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Use the reconstituted Lambda draws from the previous cell\n",
    "# This avoids re-loading the data if the cache is populated\n",
    "if 'lambda_draws' not in locals():\n",
    "    lambda_draws = load_reconstituted_lambda_draws()\n",
    "\n",
    "# Compute probabilities for each draw, then find the standard deviation\n",
    "p_draws = _sigmoid(lambda_draws)\n",
    "P_std = jnp.std(p_draws, axis=0)\n",
    "print(f\"Loaded probability draws: {p_draws.shape}; std matrix shape: {P_std.shape}\")\n",
    "\n",
    "# Plot heatmap of posterior std\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(np.asarray(P_std), cmap='magma', vmin=0.0, vmax=0.5, cbar=True,\n",
    "            xticklabels=False, yticklabels=False)\n",
    "plt.title('Posterior Std Dev of Probability (Reconstituted from Shards)')\n",
    "plt.xlabel('Diseases')\n",
    "plt.ylabel('Patients')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from dataloader import load_data\n",
    "\n",
    "# Load ground truth from dataloader and align with predicted probabilities\n",
    "\n",
    "def load_ground_truth_from_dataloader() -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load A from dataloader, convert to 2D patient-by-condition binary matrix in {0,1}.\n",
    "    \"\"\"\n",
    "    A, _, _ = load_data()\n",
    "    A_2d = A[:, :, 0] if A.ndim == 3 else A\n",
    "    if np.min(A_2d) < 0:\n",
    "        A_2d = (A_2d + 1) / 2\n",
    "    return A_2d.astype(np.int32)\n",
    "\n",
    "# Use the P_mean computed in the previous cell\n",
    "if 'P_mean' not in locals():\n",
    "     raise NameError(\"'P_mean' not defined. Please run the cell that computes the posterior mean probability first.\")\n",
    "\n",
    "A_true_full = load_ground_truth_from_dataloader()\n",
    "A_true = A_true_full[:P_mean.shape[0], :P_mean.shape[1]]\n",
    "\n",
    "# Binarize predictions at 0.5 threshold\n",
    "A_pred = (P_mean >= 0.5).astype(np.int32)\n",
    "\n",
    "# Flatten for confusion matrix computation\n",
    "y_true = A_true.flatten()\n",
    "y_pred = np.asarray(A_pred).flatten()\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Optionally, print classification report for more detail\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred, digits=3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analysis of Beta Parameters from Combined Results\n",
    "import os\n",
    "import glob\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_beta_samples_from_combined(base_dir: str = 'Res/'):\n",
    "    \"\"\"\n",
    "    Loads beta parameter samples from the latest combined results directory.\n",
    "    It reconstructs 'beta_cond' if it is not directly available.\n",
    "    \"\"\"\n",
    "    combined_dirs = sorted(glob.glob(os.path.join(base_dir, 'sharded_hierarchical_gp_*')))\n",
    "    if not combined_dirs:\n",
    "        raise FileNotFoundError(f\"No 'sharded_hierarchical_gp_*' directories found in '{base_dir}'.\")\n",
    "    \n",
    "    latest_combined_dir = combined_dirs[-1]\n",
    "    print(f\"Loading beta parameters from: {latest_combined_dir}\")\n",
    "    \n",
    "    payload = np.load(os.path.join(latest_combined_dir, 'combined_post_samples.npy'), allow_pickle=True).item()\n",
    "    \n",
    "    if 'beta_cond' in payload:\n",
    "        beta_samples = jnp.asarray(payload['beta_cond'])\n",
    "    elif 'tau' in payload and 'lambdas' in payload:\n",
    "        tau = jnp.asarray(payload['tau']).reshape(-1, 1)\n",
    "        lambdas = jnp.asarray(payload['lambdas'])\n",
    "        beta_samples = tau * lambdas\n",
    "    else:\n",
    "        raise KeyError(\"'beta_cond' or ('tau' and 'lambdas') not found in combined results.\")\n",
    "        \n",
    "    return beta_samples\n",
    "\n",
    "# Load the beta samples\n",
    "try:\n",
    "    beta_samples = load_beta_samples_from_combined()\n",
    "    \n",
    "    num_betas = beta_samples.shape[1]\n",
    "\n",
    "    # Compute posterior mean and 95% credible interval\n",
    "    beta_means = jnp.mean(beta_samples, axis=0)\n",
    "    beta_lower, beta_upper = jnp.percentile(beta_samples, jnp.array([2.5, 97.5]), axis=0)\n",
    "\n",
    "    # Compute error bars\n",
    "    yerr = jnp.vstack([beta_means - beta_lower, beta_upper - beta_means])\n",
    "\n",
    "    # Plot posterior means with error bars\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.errorbar(np.arange(num_betas), np.asarray(beta_means), yerr=np.asarray(yerr), \n",
    "                 capsize=3, capthick=1, fmt='o', markersize=4, \n",
    "                 color='blue', ecolor='gray', alpha=0.7)\n",
    "    plt.axhline(y=0, color='red', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    plt.xlabel('Beta Index (Condition)')\n",
    "    plt.ylabel('Posterior Mean')\n",
    "    plt.title('Posterior Means of Beta_Cond with 95% Credible Intervals')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(f\"\\nSummary of Beta_Cond Parameters:\")\n",
    "    print(f\"Number of conditions: {num_betas}\")\n",
    "    print(f\"Mean of all beta means: {jnp.mean(beta_means):.4f}\")\n",
    "    \n",
    "    excludes_zero = jnp.sum((beta_lower > 0) | (beta_upper < 0))\n",
    "    print(f\"Number of betas with 95% CI excluding zero: {excludes_zero}\")\n",
    "\n",
    "except (FileNotFoundError, KeyError) as e:\n",
    "    print(f\"Skipping beta analysis: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "polypharm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
